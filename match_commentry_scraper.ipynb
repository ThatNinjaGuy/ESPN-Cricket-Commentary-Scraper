{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new virtual environment\n",
    "# python3 -m venv venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./venv/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in ./venv/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: selenium in ./venv/lib/python3.12/site-packages (4.21.0)\n",
      "Requirement already satisfied: webdriver-manager in ./venv/lib/python3.12/site-packages (4.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests) (2024.6.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./venv/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: trio~=0.17 in ./venv/lib/python3.12/site-packages (from selenium) (0.25.1)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in ./venv/lib/python3.12/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in ./venv/lib/python3.12/site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: python-dotenv in ./venv/lib/python3.12/site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from webdriver-manager) (24.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in ./venv/lib/python3.12/site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in ./venv/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in ./venv/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in ./venv/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in ./venv/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in ./venv/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in ./venv/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 URLs to scrape\n",
      "Found 0 URLs already scraped\n",
      "Found 32 URLs to scrape in this iteration\n",
      "Processing URL 1/32: https://www.espncricinfo.com/series/icc-men-s-t20-world-cup-2024-1411166/united-states-of-america-vs-canada-1st-match-group-a-1415701/ball-by-ball-commentary\n",
      "Fetching URL: https://www.espncricinfo.com/series/icc-men-s-t20-world-cup-2024-1411166/united-states-of-america-vs-canada-1st-match-group-a-1415701/ball-by-ball-commentary\n",
      "Clicking on item 0: CAN\n",
      "Scroll Attempts: 6, Scrolls Successful: 5\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 250\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mProcessing URL \u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(unprocessed_urls)\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    249\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m     soups \u001b[39m=\u001b[39m extract_html_content(idx, url)\n\u001b[1;32m    251\u001b[0m     \u001b[39mfor\u001b[39;00m soup_idx, soup \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(soups):\n\u001b[1;32m    252\u001b[0m         data \u001b[39m=\u001b[39m extract_commentary_data(soup_idx, soup, url)\n",
      "Cell \u001b[0;32mIn[4], line 61\u001b[0m, in \u001b[0;36mextract_html_content\u001b[0;34m(idx, url)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[1;32m     60\u001b[0m     key \u001b[39m=\u001b[39m Keys\u001b[39m.\u001b[39mARROW_DOWN\n\u001b[0;32m---> 61\u001b[0m     driver\u001b[39m.\u001b[39;49mfind_element(By\u001b[39m.\u001b[39;49mTAG_NAME, \u001b[39m'\u001b[39;49m\u001b[39mbody\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39msend_keys(key)\n\u001b[1;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m8\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     63\u001b[0m         time\u001b[39m.\u001b[39msleep(\u001b[39m0.5\u001b[39m)  \u001b[39m# Wait for new content to load\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Code/Match Commentry Scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:741\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    738\u001b[0m     by \u001b[39m=\u001b[39m By\u001b[39m.\u001b[39mCSS_SELECTOR\n\u001b[1;32m    739\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[name=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 741\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mFIND_ELEMENT, {\u001b[39m\"\u001b[39;49m\u001b[39musing\u001b[39;49m\u001b[39m\"\u001b[39;49m: by, \u001b[39m\"\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m\"\u001b[39;49m: value})[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Code/Match Commentry Scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:345\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m params:\n\u001b[1;32m    343\u001b[0m         params[\u001b[39m\"\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_id\n\u001b[0;32m--> 345\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcommand_executor\u001b[39m.\u001b[39;49mexecute(driver_command, params)\n\u001b[1;32m    346\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m    347\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_handler\u001b[39m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/Desktop/Code/Match Commentry Scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py:302\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    300\u001b[0m trimmed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trim_large_entries(params)\n\u001b[1;32m    301\u001b[0m LOGGER\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, command_info[\u001b[39m0\u001b[39m], url, \u001b[39mstr\u001b[39m(trimmed))\n\u001b[0;32m--> 302\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(command_info[\u001b[39m0\u001b[39;49m], url, body\u001b[39m=\u001b[39;49mdata)\n",
      "File \u001b[0;32m~/Desktop/Code/Match Commentry Scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py:322\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    319\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 322\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    323\u001b[0m     statuscode \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus\n\u001b[1;32m    324\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Code/Match Commentry Scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py:144\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_url(\n\u001b[1;32m    137\u001b[0m         method,\n\u001b[1;32m    138\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw,\n\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_encode_body(\n\u001b[1;32m    145\u001b[0m         method, url, fields\u001b[39m=\u001b[39;49mfields, headers\u001b[39m=\u001b[39;49mheaders, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49murlopen_kw\n\u001b[1;32m    146\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Code/Match Commentry Scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py:279\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    275\u001b[0m     extra_kw[\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mContent-Type\u001b[39m\u001b[39m\"\u001b[39m, content_type)\n\u001b[1;32m    277\u001b[0m extra_kw\u001b[39m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 279\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw)\n",
      "File \u001b[0;32m~/Desktop/Code/Match Commentry Scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py:444\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    442\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m    443\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(method, u\u001b[39m.\u001b[39;49mrequest_uri, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    446\u001b[0m redirect_location \u001b[39m=\u001b[39m redirect \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mget_redirect_location()\n\u001b[1;32m    447\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/Desktop/Code/Match Commentry Scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    794\u001b[0m     conn,\n\u001b[1;32m    795\u001b[0m     method,\n\u001b[1;32m    796\u001b[0m     url,\n\u001b[1;32m    797\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    798\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    799\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    800\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    801\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    802\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    803\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    804\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    805\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    806\u001b[0m )\n\u001b[1;32m    808\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Code/Match Commentry Scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    538\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/Desktop/Code/Match Commentry Scraper/venv/lib/python3.12/site-packages/urllib3/connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    465\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    468\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1429\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    332\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    708\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def extract_html_content(idx, url):\n",
    "    print(f'Fetching URL: {url}')\n",
    "    \n",
    "    # Set up the Selenium WebDriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run in headless mode for faster execution\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    \n",
    "    soups = []\n",
    "    dropdown_xpath = '//div[contains(@class, \"ds-flex ds-items-center ds-border-ui-stroke ds-h-6 ds-px-4 ds-border ds-bg-ui-fill ds-rounded-full ds-w-full ds-min-w-max ds-cursor-pointer\")]'\n",
    "    dropdown_items_xpath = '//ul[contains(@class, \"ds-flex ds-flex-col ds-text-typo-mid2 ds-justify-center ds-overflow-ellipsis ds-overflow-y-auto ds-w-full ds-grid ds-grid-cols-1 ds-items-center ds-gap-x-2 ds-max-h-96 ds-overflow-y-auto\")]/li/div'\n",
    "    \n",
    "    # Find and click the dropdown to expand it\n",
    "    dropdown = driver.find_element(By.XPATH, dropdown_xpath)\n",
    "    driver.execute_script(\"arguments[0].click();\", dropdown)\n",
    "    time.sleep(2)  # Wait for the dropdown to open\n",
    "    \n",
    "    # Find all items in the dropdown\n",
    "    dropdown_items = driver.find_elements(By.XPATH, dropdown_items_xpath)\n",
    "    \n",
    "    for item_idx, item in enumerate(dropdown_items):\n",
    "        try:\n",
    "            print(f'Clicking on item {item_idx}: {item.text.strip()}')  # Print the item text for debugging\n",
    "            driver.execute_script(\"arguments[0].click();\", item)\n",
    "            time.sleep(2)  # Wait for the page to load\n",
    "\n",
    "            # Scroll until no more content is loaded\n",
    "            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            scroll_attempts = 0\n",
    "            scroll_successful = 0\n",
    "            total_scroll_attempts = 0\n",
    "            # screenshot_count = 0\n",
    "\n",
    "            max_retries = 3\n",
    "            flow_retry = 0\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    retries = 0\n",
    "                    while retries < max_retries:\n",
    "                        try:\n",
    "                            # Scroll down using large scrolls\n",
    "                            driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)\n",
    "                            time.sleep(2)  # Wait for new content to load\n",
    "\n",
    "                            total_scroll_attempts += 1\n",
    "                            # Scroll up and down to load content in the viewport\n",
    "                            for i in range(150):\n",
    "                                key = Keys.ARROW_UP\n",
    "                                driver.find_element(By.TAG_NAME, 'body').send_keys(key)\n",
    "                                if i % 8 == 0:\n",
    "                                    time.sleep(0.5)  # Wait for new content to load\n",
    "\n",
    "                            # Fine-tune scrolling to ensure all content is loaded\n",
    "                            for i in range(100):\n",
    "                                key = Keys.ARROW_DOWN\n",
    "                                driver.find_element(By.TAG_NAME, 'body').send_keys(key)\n",
    "                                if i % 8 == 0:\n",
    "                                    time.sleep(0.5)  # Wait for new content to load\n",
    "\n",
    "                            # Capture a screenshot after scrolling\n",
    "                            # screenshot_path = f'screenshot_{idx}_{item_idx}_{screenshot_count}.png'\n",
    "                            # driver.save_screenshot(screenshot_path)\n",
    "                            # print(f'Screenshot saved at {screenshot_path}')\n",
    "                            # screenshot_count += 1\n",
    "\n",
    "                            # Check if we reached the end of the page\n",
    "                            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                            if new_height == last_height:\n",
    "                                scroll_attempts += 1\n",
    "                                if scroll_attempts >= 2:\n",
    "                                    break\n",
    "                            else:\n",
    "                                scroll_attempts = 0\n",
    "                                scroll_successful += 1\n",
    "\n",
    "                            last_height = new_height\n",
    "\n",
    "                            # Print progress\n",
    "                            print(f'Scroll Attempts: {total_scroll_attempts}, Scrolls Successful: {scroll_successful}', end='\\r')\n",
    "                            retries = 0  # Reset retries after a successful attempt\n",
    "                        except Exception as e:\n",
    "                            retries += 1\n",
    "                            if retries >= max_retries:\n",
    "                                print(f\"Error occurred: {e}. Retried {max_retries} times. Exiting...\")\n",
    "                                raise\n",
    "                            else:\n",
    "                                print(f\"Error occurred: {e}. Retrying {retries}/{max_retries}...\", end='\\r')\n",
    "                                time.sleep(2)  # Wait before retrying\n",
    "                except Exception as e:\n",
    "                    # Handle any cleanup or final error logging here if needed\n",
    "                    print(f\"Final error after {max_retries} retries: {e}\")\n",
    "                    break\n",
    "            # Get the page source and parse the HTML content with BeautifulSoup\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            soups.append(soup)\n",
    "\n",
    "            # Click the dropdown again to select the next item\n",
    "            dropdown = driver.find_element(By.XPATH, dropdown_xpath)\n",
    "            driver.execute_script(\"arguments[0].click();\", dropdown)\n",
    "            time.sleep(2)  # Wait for the dropdown to open\n",
    "            dropdown_items = driver.find_elements(By.XPATH, dropdown_items_xpath)\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing the click of dropdown for dropdown item index:{item_idx} - {e.message}\")\n",
    "\n",
    "    driver.quit()\n",
    "    print(f'\\nSuccessfully fetched and parsed URL: {url}')\n",
    "    return soups\n",
    "\n",
    "\n",
    "def extract_match_details(url):\n",
    "    # Updated regular expression to capture the match details\n",
    "    pattern = re.compile(\n",
    "        r\"https://www\\.espncricinfo\\.com/series/icc-men-s-t20-world-cup-2024-1411166/([a-zA-Z-]+)-vs-([a-zA-Z-]+)-(\\d+)[a-z]{2}-match-group-([a-z])-([0-9]+)/ball-by-ball-commentary\"\n",
    "    )\n",
    "    match = pattern.match(url)\n",
    "    if match:\n",
    "        details = match.groups()\n",
    "        print(f\"Match details found: {details}\")  # Debug print\n",
    "\n",
    "        team_1 = details[0].replace('-', ' ').title()\n",
    "        team_2 = details[1].replace('-', ' ').title()\n",
    "        match_number = details[2]\n",
    "        group_id = details[3].upper()\n",
    "        match_id = details[4]\n",
    "\n",
    "        return team_1, team_2, match_number, match_id, group_id\n",
    "    else:\n",
    "        print(\"No match found.\")  # Debug print\n",
    "        return None\n",
    "\n",
    "def extract_commentary_data(idx, soup, url):\n",
    "    print('Extracting commentary data')\n",
    "    data = []\n",
    "    # Extract match details\n",
    "    team_1, team_2, match_number, match_id, group_id = extract_match_details(url)\n",
    "    # Find all commentary blocks\n",
    "    commentary_blocks = soup.find_all('div', class_='ds-text-tight-m ds-font-regular ds-flex ds-px-3 ds-py-2 lg:ds-px-4 lg:ds-py-[10px] ds-items-start ds-select-none lg:ds-select-auto')\n",
    "    print(f'Found {len(commentary_blocks)} commentary blocks')\n",
    "\n",
    "    for index, block in enumerate(commentary_blocks, start=1):\n",
    "        try:\n",
    "            # Extract the over\n",
    "            over_elem = block.find('span', class_='ds-text-tight-s ds-font-regular ds-mb-1 lg:ds-mb-0 lg:ds-mr-3 ds-block ds-text-center ds-text-typo-mid1')\n",
    "            over = over_elem.text.strip() if over_elem else None\n",
    "            \n",
    "            # Extract the runs\n",
    "            runs_block = block.find('div', class_='lg:ds-flex lg:ds-items-center lg:ds-px-2')\n",
    "            runs = None\n",
    "            if runs_block:\n",
    "                runs = runs_block.find('div', class_='ds-flex ds-items-center ds-justify-center ds-rounded ds-overflow-hidden ds-bg-raw-green-d2 ds-text-raw-white') or \\\n",
    "                runs_block.find('div', class_='ds-text-tight-m ds-font-bold ds-flex ds-items-center ds-justify-center ds-text-center ds-w-10 ds-h-10 ds-text-raw-white') or \\\n",
    "                runs_block.find('div', class_='ds-flex ds-items-center ds-justify-center ds-rounded ds-overflow-hidden ds-bg-ui-fill-default-translucent ds-text-typo') or \\\n",
    "                runs_block.find('div', class_='ds-flex ds-items-center ds-justify-center ds-rounded ds-overflow-hidden ds-bg-raw-red ds-text-raw-white') or \\\n",
    "                runs_block.find('div', class_='ds-flex ds-items-center ds-justify-center ds-rounded ds-overflow-hidden ds-bg-ui-fill-default-translucent ds-text-typo') or \\\n",
    "                runs_block.find('div', class_='ds-flex ds-items-center ds-justify-center ds-rounded ds-overflow-hidden ds-bg-raw-purple ds-text-raw-white')\n",
    "                runs = runs.find('span').text.strip() if runs else None\n",
    "            \n",
    "            # Extract the main message\n",
    "            main_message_elem = block.find('div', class_='ds-leading-none ds-mb-0.5')\n",
    "            main_message = main_message_elem.find('span').text.strip() if main_message_elem else None\n",
    "            \n",
    "            # Extract the complete commentary\n",
    "            complete_commentary_elem = block.find('div', class_='first-letter:ds-capitalize').find('p', class_='ci-html-content')\n",
    "            complete_commentary = complete_commentary_elem.text.strip() if complete_commentary_elem else None\n",
    "            \n",
    "            # Append the extracted data to the list\n",
    "            data.append({\n",
    "                'Match Id': match_id,\n",
    "                'Match Number': match_number,\n",
    "                'Group Number': group_id,\n",
    "                'Team A': team_1,\n",
    "                'Team B': team_2,\n",
    "                'Innings': idx+1,\n",
    "                'Over': over,\n",
    "                'Runs': runs,\n",
    "                'Main Message': main_message,\n",
    "                'Complete Commentary': complete_commentary\n",
    "            })\n",
    "\n",
    "            # Print progress for commentary processing\n",
    "            print(f'Processing progress for commentary: {index}/{len(commentary_blocks)}', end='\\r')\n",
    "        except Exception as e:\n",
    "            print(f'Skipping block {block} due to error: {e}')\n",
    "    \n",
    "    print('\\nCompleted extracting commentary data')\n",
    "    return data\n",
    "\n",
    "def get_processed_data(data, file_appender=''):\n",
    "    print('Processing data into DataFrame')\n",
    "    columns = ['Match Id', 'Match Number', 'Group Number', 'Team A', 'Team B', 'Innings', 'Over', 'Runs', 'Main Message', 'Complete Commentary']\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    file_name = f'commentary_results_{file_appender}.csv'\n",
    "    \n",
    "    try:\n",
    "        existing_df = pd.read_csv(file_name)\n",
    "        combined_df = pd.concat([existing_df, df]).drop_duplicates().reset_index(drop=True)\n",
    "    except FileNotFoundError:\n",
    "        combined_df = df\n",
    "    \n",
    "    combined_df.to_csv(file_name, index=False)\n",
    "    print(f'Data saved to {file_name}')\n",
    "    return combined_df\n",
    "\n",
    "def get_urls_to_scrape(csv_file='urls_to_scrape.csv'):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # Extract the URLs column into a list\n",
    "    urls = df['Urls'].tolist()\n",
    "    return urls\n",
    "\n",
    "def get_processed_urls(processed_file='processed_urls.csv'):\n",
    "    # Read the processed URLs from the CSV file\n",
    "    try:\n",
    "        processed_df = pd.read_csv(processed_file)\n",
    "        return processed_df['URL'].tolist()\n",
    "    except FileNotFoundError:\n",
    "        return []\n",
    "\n",
    "def save_processed_url(processed_file, url):\n",
    "    # Append the new processed URL to the processed URLs CSV file\n",
    "    new_entry_df = pd.DataFrame([url], columns=['URL'])\n",
    "    try:\n",
    "        existing_df = pd.read_csv(processed_file)\n",
    "        combined_df = pd.concat([existing_df, new_entry_df]).drop_duplicates().reset_index(drop=True)\n",
    "    except FileNotFoundError:\n",
    "        combined_df = new_entry_df\n",
    "    combined_df.to_csv(processed_file, index=False)\n",
    "\n",
    "\n",
    "combined_data = []\n",
    "urls_to_scrape_file = 'urls_to_scrape.csv'\n",
    "processed_urls_file = 'processed_urls.csv'\n",
    "\n",
    "urls = get_urls_to_scrape(urls_to_scrape_file)\n",
    "print(f'Found {len(urls)} URLs to scrape')\n",
    "processed_urls = get_processed_urls(processed_urls_file)\n",
    "print(f'Found {len(processed_urls)} URLs already scraped')\n",
    "unprocessed_urls = [url for url in urls if url not in processed_urls]\n",
    "print(f'Found {len(unprocessed_urls)} URLs to scrape in this iteration')\n",
    "\n",
    "for idx, url in enumerate(unprocessed_urls):\n",
    "    print(f'Processing URL {idx + 1}/{len(unprocessed_urls)}: {url}')\n",
    "    try:\n",
    "        soups = extract_html_content(idx, url)\n",
    "        for soup_idx, soup in enumerate(soups):\n",
    "            data = extract_commentary_data(soup_idx, soup, url)\n",
    "            combined_data.extend(data)\n",
    "            get_processed_data(data, 'partial')\n",
    "        # Save the processed URL to the processed URLs CSV file\n",
    "        save_processed_url(processed_urls_file, url)\n",
    "        get_processed_data(combined_data, 'final')\n",
    "        combined_data = []  # Clear combined_data after saving\n",
    "    except Exception as e:\n",
    "        print(f'Error processing URL {url}: {e}')\n",
    "\n",
    "# Save any remaining data\n",
    "if combined_data:\n",
    "    df = get_processed_data(combined_data, 'final')\n",
    "else:\n",
    "    df = pd.DataFrame(columns=['Match Id', 'Match Number', 'Group Number', 'Team A', 'Team B', 'Innings', 'Over', 'Runs', 'Main Message', 'Complete Commentary'])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
